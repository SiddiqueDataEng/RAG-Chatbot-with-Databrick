{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise RAG Chatbot - Data Pipeline\n",
    "## Complete End-to-End Data Engineering Pipeline\n",
    "\n",
    "This notebook demonstrates the complete data engineering pipeline for the RAG chatbot:\n",
    "1. Data Ingestion from multiple sources\n",
    "2. Text Processing and Cleaning\n",
    "3. Intelligent Chunking\n",
    "4. Embedding Generation\n",
    "5. Vector Store Creation\n",
    "6. Quality Validation\n",
    "\n",
    "### Prerequisites\n",
    "- Databricks environment with Spark\n",
    "- Access to Foundation Models\n",
    "- Vector Search endpoint configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install transformers==4.30.2 \"unstructured[pdf,docx]==0.10.30\" langchain==0.0.319 llama-index==0.9.3 databricks-vectorsearch==0.20 pydantic==1.10.9 mlflow==2.9.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Core imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Local imports\n",
    "from data_engineering.data_ingestion import DataIngestionPipeline\n",
    "from data_engineering.text_processing import TextProcessor, TextQualityMonitor\n",
    "from feature_engineering.embeddings import EmbeddingGenerator, FeatureEngineer, EmbeddingQualityValidator\n",
    "from modeling.vector_store import VectorStoreManager\n",
    "from utils.config_manager import ConfigManager\n",
    "from utils.logging_utils import setup_logging, get_logger\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "setup_logging(log_level=\"INFO\", log_format=\"structured\")\n",
    "logger = get_logger(\"data_pipeline\", {\"notebook\": \"01_data_pipeline\"})\n",
    "\n",
    "# Load configuration\n",
    "config_manager = ConfigManager()\n",
    "config = config_manager.load_config()\n",
    "\n",
    "# Display configuration summary\n",
    "config_summary = config_manager.get_config_summary()\n",
    "print(\"Configuration Summary:\")\n",
    "for key, value in config_summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "logger.info(\"Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with optimized settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(config['infrastructure']['spark']['app_name']) \\\n",
    "    .config(\"spark.executor.memory\", config['infrastructure']['spark']['executor_memory']) \\\n",
    "    .config(\"spark.driver.memory\", config['infrastructure']['spark']['driver_memory']) \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set MLflow experiment\n",
    "mlflow.set_tracking_uri(config['infrastructure']['mlflow']['tracking_uri'])\n",
    "mlflow.set_experiment(config['infrastructure']['mlflow']['experiment_name'])\n",
    "\n",
    "print(f\"Spark session initialized: {spark.version}\")\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "logger.info(\"Spark and MLflow initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data ingestion pipeline\n",
    "ingestion_pipeline = DataIngestionPipeline(config, spark)\n",
    "\n",
    "# Define data sources\n",
    "arxiv_papers = [\n",
    "    '2312.14565',  # Recent LLM research\n",
    "    '2303.10130',  # GPT-4 technical report\n",
    "    '2302.06476',  # LLaMA paper\n",
    "    '2311.07071',  # Advanced RAG techniques\n",
    "    '2304.07683',  # Vector databases\n",
    "    '2310.06825',  # Retrieval augmented generation\n",
    "    '2309.15217',  # Large language models\n",
    "    '2308.07107'   # Embedding models\n",
    "]\n",
    "\n",
    "# Additional URLs for diverse content\n",
    "additional_urls = [\n",
    "    'https://arxiv.org/pdf/2312.00506.pdf',  # AI safety\n",
    "    'https://arxiv.org/pdf/2311.16867.pdf'   # Multimodal AI\n",
    "]\n",
    "\n",
    "logger.info(f\"Configured {len(arxiv_papers)} arXiv papers and {len(additional_urls)} additional URLs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run for data pipeline\n",
    "with mlflow.start_run(run_name=\"data_pipeline_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    \n",
    "    # Log pipeline parameters\n",
    "    mlflow.log_params({\n",
    "        \"num_arxiv_papers\": len(arxiv_papers),\n",
    "        \"num_additional_urls\": len(additional_urls),\n",
    "        \"chunk_size\": config['data']['chunk_size'],\n",
    "        \"chunk_overlap\": config['data']['chunk_overlap'],\n",
    "        \"embedding_model\": config['models']['embedding']['name'],\n",
    "        \"embedding_dimension\": config['models']['embedding']['dimension']\n",
    "    })\n",
    "    \n",
    "    # Step 1: Ingest arXiv papers\n",
    "    logger.info(\"Starting arXiv paper ingestion\")\n",
    "    df_arxiv = ingestion_pipeline.ingest_arxiv_papers(arxiv_papers)\n",
    "    \n",
    "    # Step 2: Ingest additional URLs\n",
    "    logger.info(\"Starting additional URL ingestion\")\n",
    "    df_urls = ingestion_pipeline.ingest_from_urls(additional_urls)\n",
    "    \n",
    "    # Step 3: Combine datasets\n",
    "    df_raw = df_arxiv.union(df_urls)\n",
    "    \n",
    "    # Cache for performance\n",
    "    df_raw.cache()\n",
    "    \n",
    "    # Display ingestion results\n",
    "    total_docs = df_raw.count()\n",
    "    print(f\"\\nIngestion Results:\")\n",
    "    print(f\"Total documents ingested: {total_docs}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    display(df_raw.select(\"file_name\", \"file_type\", \"file_size\", \"source_type\").limit(10))\n",
    "    \n",
    "    # Log ingestion metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"total_documents_ingested\": total_docs,\n",
    "        \"arxiv_documents\": df_arxiv.count(),\n",
    "        \"url_documents\": df_urls.count()\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Ingestion completed: {total_docs} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data quality\n",
    "quality_report = ingestion_pipeline.validate_data_quality(df_raw)\n",
    "\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(json.dumps(quality_report, indent=2))\n",
    "\n",
    "# Log quality metrics\n",
    "mlflow.log_metrics({\n",
    "    \"duplicate_percentage\": quality_report['duplicate_percentage'],\n",
    "    \"unique_documents\": quality_report['unique_documents']\n",
    "})\n",
    "\n",
    "# Save raw data to Delta table\n",
    "table_name = \"rag_documents_raw\"\n",
    "ingestion_pipeline.save_to_delta_table(df_raw, table_name, mode=\"overwrite\")\n",
    "\n",
    "logger.info(f\"Raw data saved to table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text processor\n",
    "text_processor = TextProcessor(config)\n",
    "quality_monitor = TextQualityMonitor()\n",
    "\n",
    "logger.info(\"Text processor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract text from documents\n",
    "logger.info(\"Starting text extraction\")\n",
    "df_with_text = text_processor.extract_text_from_documents(df_raw)\n",
    "\n",
    "# Show text extraction results\n",
    "print(\"\\nText Extraction Results:\")\n",
    "df_text_stats = df_with_text.select(\n",
    "    \"file_name\", \n",
    "    \"text_length\", \n",
    "    \"word_count\"\n",
    ").orderBy(desc(\"text_length\"))\n",
    "\n",
    "display(df_text_stats.limit(10))\n",
    "\n",
    "# Log text extraction metrics\n",
    "avg_text_length = df_with_text.agg({\"text_length\": \"avg\"}).collect()[0][0]\n",
    "avg_word_count = df_with_text.agg({\"word_count\": \"avg\"}).collect()[0][0]\n",
    "\n",
    "mlflow.log_metrics({\n",
    "    \"avg_text_length\": avg_text_length or 0,\n",
    "    \"avg_word_count\": avg_word_count or 0\n",
    "})\n",
    "\n",
    "logger.info(f\"Text extraction completed. Avg length: {avg_text_length:.0f} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Clean and preprocess text\n",
    "logger.info(\"Starting text cleaning\")\n",
    "df_cleaned = text_processor.clean_and_preprocess_text(df_with_text)\n",
    "\n",
    "# Show cleaning results\n",
    "docs_before = df_with_text.count()\n",
    "docs_after = df_cleaned.count()\n",
    "\n",
    "print(f\"\\nText Cleaning Results:\")\n",
    "print(f\"Documents before cleaning: {docs_before}\")\n",
    "print(f\"Documents after cleaning: {docs_after}\")\n",
    "print(f\"Documents filtered out: {docs_before - docs_after}\")\n",
    "\n",
    "# Sample cleaned text\n",
    "sample_text = df_cleaned.select(\"file_name\", \"cleaned_text\").limit(1).collect()[0]\n",
    "print(f\"\\nSample cleaned text from {sample_text.file_name}:\")\n",
    "print(sample_text.cleaned_text[:500] + \"...\")\n",
    "\n",
    "mlflow.log_metrics({\n",
    "    \"documents_after_cleaning\": docs_after,\n",
    "    \"cleaning_filter_rate\": (docs_before - docs_after) / docs_before if docs_before > 0 else 0\n",
    "})\n",
    "\n",
    "logger.info(f\"Text cleaning completed. {docs_after} documents retained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create intelligent chunks\n",
    "logger.info(\"Starting intelligent chunking\")\n",
    "df_chunks = text_processor.create_intelligent_chunks(df_cleaned)\n",
    "\n",
    "# Cache chunks for performance\n",
    "df_chunks.cache()\n",
    "\n",
    "# Show chunking results\n",
    "total_chunks = df_chunks.count()\n",
    "print(f\"\\nChunking Results:\")\n",
    "print(f\"Total chunks created: {total_chunks}\")\n",
    "\n",
    "# Chunk statistics\n",
    "chunk_stats = df_chunks.groupBy(\"chunk_type\").count().orderBy(desc(\"count\"))\n",
    "print(\"\\nChunk type distribution:\")\n",
    "display(chunk_stats)\n",
    "\n",
    "# Token count statistics\n",
    "token_stats = df_chunks.select(\"token_count\").describe()\n",
    "print(\"\\nToken count statistics:\")\n",
    "display(token_stats)\n",
    "\n",
    "# Sample chunks\n",
    "print(\"\\nSample chunks:\")\n",
    "sample_chunks = df_chunks.select(\n",
    "    \"file_name\", \"chunk_id\", \"chunk_type\", \"token_count\", \"chunk_content\"\n",
    ").limit(3).collect()\n",
    "\n",
    "for chunk in sample_chunks:\n",
    "    print(f\"\\nFile: {chunk.file_name}, Chunk: {chunk.chunk_id}, Type: {chunk.chunk_type}, Tokens: {chunk.token_count}\")\n",
    "    print(f\"Content: {chunk.chunk_content[:200]}...\")\n",
    "\n",
    "mlflow.log_metrics({\n",
    "    \"total_chunks_created\": total_chunks,\n",
    "    \"avg_chunks_per_document\": total_chunks / docs_after if docs_after > 0 else 0\n",
    "})\n",
    "\n",
    "logger.info(f\"Chunking completed: {total_chunks} chunks created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate chunk quality\n",
    "chunk_quality = text_processor.validate_chunk_quality(df_chunks)\n",
    "\n",
    "print(\"\\nChunk Quality Report:\")\n",
    "print(json.dumps(chunk_quality, indent=2))\n",
    "\n",
    "# Log chunk quality metrics\n",
    "mlflow.log_metrics({\n",
    "    \"chunk_quality_score\": 1.0 if chunk_quality['validation_passed'] else 0.0,\n",
    "    \"short_chunks_percentage\": chunk_quality['short_chunks_percentage'],\n",
    "    \"avg_tokens_per_chunk\": chunk_quality['average_tokens_per_chunk']\n",
    "})\n",
    "\n",
    "# Track processing metrics\n",
    "processing_metrics = quality_monitor.track_processing_metrics(df_with_text, df_chunks)\n",
    "print(\"\\nProcessing Metrics:\")\n",
    "print(json.dumps(processing_metrics, indent=2))\n",
    "\n",
    "logger.info(\"Text processing quality validation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineering components\n",
    "embedding_generator = EmbeddingGenerator(config)\n",
    "feature_engineer = FeatureEngineer(config)\n",
    "embedding_validator = EmbeddingQualityValidator()\n",
    "\n",
    "logger.info(\"Feature engineering components initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate embeddings\n",
    "logger.info(\"Starting embedding generation\")\n",
    "df_embeddings = embedding_generator.generate_embeddings(df_chunks, \"chunk_content\")\n",
    "\n",
    "# Cache embeddings for performance\n",
    "df_embeddings.cache()\n",
    "\n",
    "# Show embedding results\n",
    "embeddings_count = df_embeddings.count()\n",
    "print(f\"\\nEmbedding Generation Results:\")\n",
    "print(f\"Total embeddings generated: {embeddings_count}\")\n",
    "\n",
    "# Sample embedding\n",
    "sample_embedding = df_embeddings.select(\"file_name\", \"chunk_content\", \"embedding\").limit(1).collect()[0]\n",
    "print(f\"\\nSample embedding from {sample_embedding.file_name}:\")\n",
    "print(f\"Content: {sample_embedding.chunk_content[:100]}...\")\n",
    "print(f\"Embedding dimension: {len(sample_embedding.embedding)}\")\n",
    "print(f\"Embedding sample: {sample_embedding.embedding[:5]}...\")\n",
    "\n",
    "mlflow.log_metrics({\n",
    "    \"embeddings_generated\": embeddings_count,\n",
    "    \"embedding_dimension\": len(sample_embedding.embedding)\n",
    "})\n",
    "\n",
    "logger.info(f\"Embedding generation completed: {embeddings_count} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Validate embedding quality\n",
    "embedding_quality = embedding_validator.validate_embeddings(df_embeddings)\n",
    "\n",
    "print(\"\\nEmbedding Quality Report:\")\n",
    "print(json.dumps(embedding_quality, indent=2))\n",
    "\n",
    "# Log embedding quality metrics\n",
    "mlflow.log_metrics({\n",
    "    \"embedding_quality_score\": embedding_quality['quality_score'],\n",
    "    \"null_embeddings_percentage\": embedding_quality['null_percentage'],\n",
    "    \"dimension_consistency\": 1.0 if embedding_quality['dimension_consistency'] else 0.0\n",
    "})\n",
    "\n",
    "logger.info(f\"Embedding quality validation completed. Score: {embedding_quality['quality_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create additional features\n",
    "logger.info(\"Creating additional text features\")\n",
    "df_with_features = feature_engineer.create_text_features(df_embeddings)\n",
    "\n",
    "# Add semantic features\n",
    "logger.info(\"Creating semantic features\")\n",
    "df_with_semantic = feature_engineer.create_semantic_features(df_with_features)\n",
    "\n",
    "# Add contextual features\n",
    "logger.info(\"Creating contextual features\")\n",
    "df_final = feature_engineer.create_contextual_features(df_with_semantic)\n",
    "\n",
    "# Show feature engineering results\n",
    "print(\"\\nFeature Engineering Results:\")\n",
    "print(f\"Final dataset shape: {df_final.count()} rows\")\n",
    "\n",
    "# Sample features\n",
    "sample_features = df_final.select(\n",
    "    \"file_name\", \"chunk_id\", \"text_features\", \"semantic_features\", \"contextual_features\"\n",
    ").limit(1).collect()[0]\n",
    "\n",
    "print(f\"\\nSample features from {sample_features.file_name}:\")\n",
    "print(f\"Text features: {sample_features.text_features}\")\n",
    "print(f\"Semantic features: {sample_features.semantic_features}\")\n",
    "print(f\"Contextual features: {sample_features.contextual_features}\")\n",
    "\n",
    "logger.info(\"Feature engineering completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to Delta table for vector store\n",
    "embeddings_table = \"rag_embeddings\"\n",
    "\n",
    "# Prepare final dataset with required columns\n",
    "df_for_vector_store = df_final.select(\n",
    "    col(\"content_hash\").alias(\"chunk_id\"),  # Use content_hash as unique ID\n",
    "    col(\"chunk_content\"),\n",
    "    col(\"embedding\"),\n",
    "    col(\"file_name\"),\n",
    "    col(\"chunk_type\"),\n",
    "    col(\"token_count\"),\n",
    "    col(\"embedding_timestamp\")\n",
    ")\n",
    "\n",
    "# Save to Delta table with Change Data Feed enabled\n",
    "df_for_vector_store.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .saveAsTable(embeddings_table)\n",
    "\n",
    "print(f\"\\nEmbeddings saved to table: {embeddings_table}\")\n",
    "print(f\"Total records: {df_for_vector_store.count()}\")\n",
    "\n",
    "logger.info(f\"Embeddings table created: {embeddings_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store manager\n",
    "vector_store = VectorStoreManager(config)\n",
    "\n",
    "# Create vector index\n",
    "logger.info(\"Creating vector search index\")\n",
    "index_created = vector_store.create_index(df_for_vector_store, force_recreate=True)\n",
    "\n",
    "if index_created:\n",
    "    print(\"\\nVector index created successfully!\")\n",
    "    \n",
    "    # Get index statistics\n",
    "    index_stats = vector_store.get_index_stats()\n",
    "    print(\"\\nVector Index Statistics:\")\n",
    "    print(json.dumps(index_stats, indent=2))\n",
    "    \n",
    "    # Log vector store metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"vector_index_created\": 1.0,\n",
    "        \"vectors_indexed\": df_for_vector_store.count()\n",
    "    })\n",
    "    \n",
    "    logger.info(\"Vector index creation completed\")\n",
    "else:\n",
    "    print(\"\\nFailed to create vector index\")\n",
    "    mlflow.log_metrics({\"vector_index_created\": 0.0})\n",
    "    logger.error(\"Vector index creation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector search functionality\n",
    "if index_created:\n",
    "    logger.info(\"Testing vector search functionality\")\n",
    "    \n",
    "    # Generate test query embedding\n",
    "    test_query = \"What is machine learning and how does it work?\"\n",
    "    \n",
    "    # For testing, we'll use a sample embedding from our dataset\n",
    "    sample_embedding = df_for_vector_store.select(\"embedding\").limit(1).collect()[0].embedding\n",
    "    \n",
    "    # Perform search\n",
    "    search_results = vector_store.search(\n",
    "        query_embedding=sample_embedding,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nVector Search Test Results:\")\n",
    "    print(f\"Query: {test_query}\")\n",
    "    print(f\"Results found: {len(search_results)}\")\n",
    "    \n",
    "    for i, result in enumerate(search_results[:3]):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"  Score: {result.score:.4f}\")\n",
    "        print(f\"  Document: {result.document_name}\")\n",
    "        print(f\"  Content: {result.content[:150]}...\")\n",
    "    \n",
    "    # Log search test metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"search_test_results\": len(search_results),\n",
    "        \"avg_search_score\": np.mean([r.score for r in search_results]) if search_results else 0.0\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Vector search test completed: {len(search_results)} results\")\n",
    "else:\n",
    "    print(\"\\nSkipping search test - vector index not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive pipeline report\n",
    "pipeline_report = {\n",
    "    \"pipeline_execution\": {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"status\": \"completed\",\n",
    "        \"spark_version\": spark.version\n",
    "    },\n",
    "    \"data_ingestion\": {\n",
    "        \"total_documents\": total_docs,\n",
    "        \"quality_score\": quality_report.get('duplicate_percentage', 0),\n",
    "        \"sources\": {\n",
    "            \"arxiv_papers\": len(arxiv_papers),\n",
    "            \"additional_urls\": len(additional_urls)\n",
    "        }\n",
    "    },\n",
    "    \"text_processing\": {\n",
    "        \"documents_processed\": docs_after,\n",
    "        \"total_chunks\": total_chunks,\n",
    "        \"avg_chunk_tokens\": chunk_quality.get('average_tokens_per_chunk', 0),\n",
    "        \"quality_passed\": chunk_quality.get('validation_passed', False)\n",
    "    },\n",
    "    \"feature_engineering\": {\n",
    "        \"embeddings_generated\": embeddings_count,\n",
    "        \"embedding_dimension\": config['models']['embedding']['dimension'],\n",
    "        \"quality_score\": embedding_quality.get('quality_score', 0)\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"index_created\": index_created,\n",
    "        \"provider\": config['vector_db']['provider'],\n",
    "        \"index_name\": config['vector_db']['index_name']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PIPELINE EXECUTION REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(json.dumps(pipeline_report, indent=2))\n",
    "\n",
    "# Save report as MLflow artifact\n",
    "with open(\"/tmp/pipeline_report.json\", \"w\") as f:\n",
    "    json.dump(pipeline_report, f, indent=2)\n",
    "\n",
    "mlflow.log_artifact(\"/tmp/pipeline_report.json\", \"reports\")\n",
    "\n",
    "# Log final pipeline metrics\n",
    "mlflow.log_metrics({\n",
    "    \"pipeline_success\": 1.0,\n",
    "    \"total_processing_time\": (datetime.now() - datetime.fromisoformat(pipeline_report['pipeline_execution']['timestamp'].replace('Z', '+00:00'))).total_seconds()\n",
    "})\n",
    "\n",
    "logger.info(\"Pipeline execution completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "The data pipeline has been successfully executed. Here's what was accomplished:\n",
    "\n",
    "1. **Data Ingestion**: Successfully ingested documents from arXiv and additional URLs\n",
    "2. **Text Processing**: Extracted, cleaned, and intelligently chunked text content\n",
    "3. **Feature Engineering**: Generated high-quality embeddings and additional features\n",
    "4. **Vector Store**: Created searchable vector index for similarity search\n",
    "5. **Quality Validation**: Validated data quality at each step\n",
    "\n",
    "### Ready for Next Phase:\n",
    "- **RAG Pipeline**: The vector store is ready for retrieval-augmented generation\n",
    "- **API Deployment**: Data is prepared for serving through the API\n",
    "- **Evaluation**: Ready for quality assessment and performance testing\n",
    "\n",
    "### Monitoring:\n",
    "- All metrics have been logged to MLflow for tracking\n",
    "- Quality reports are available for ongoing monitoring\n",
    "- Pipeline can be re-run with new data sources\n",
    "\n",
    "Proceed to notebook `02_rag_pipeline.ipynb` to test the complete RAG system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}